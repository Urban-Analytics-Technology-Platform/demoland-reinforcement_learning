{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0488367-0e80-4497-965c-7f2993f7325b",
   "metadata": {},
   "source": [
    "# Import the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5488ea6e-4fef-49b4-a315-b8f1b1a03d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7809dee0-ee9e-4d53-9e83-fcfb622702db",
   "metadata": {},
   "source": [
    "# Read the parquet file and split it into features/labels and train/test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd89c1c9-42e0-4450-bc61-cedba63f91b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "path2file = './all_oa.parquet'\n",
    "all_oa_df = pd.read_parquet(path2file)\n",
    "# all_oa_df.info()\n",
    "# all_oa_df.describe()\n",
    "\n",
    "# shuffle the DataFrame rows\n",
    "all_oa_df = all_oa_df.sample(frac=1, random_state=42)  # random_state not needed here cause we do that in train/test split\n",
    "\n",
    "# X = all_oa_df.loc[:, ['air_quality_index', 'house_price_index', 'jobs_accessibility_index']].to_numpy(dtype=np.float32)   # features\n",
    "X = all_oa_df.loc[:, ['air_quality_index', 'jobs_accessibility_index']].to_numpy(dtype=np.float32)   # features\n",
    "# Y = all_oa_df[all_oa_df.columns.difference(['air_quality_index', 'house_price_index', 'jobs_accessibility_index', 'geo_code', 'geometry'])].to_numpy(dtype=np.float32)  # labels\n",
    "Y = all_oa_df.loc[:, ['population_estimate', 'A, B, D, E. Agriculture, energy and water', 'C. Manufacturing', 'F. Construction', 'G, I. Distribution, hotels and restaurants']].to_numpy(dtype=np.float32)   # labels\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92e1091-5a36-4833-ba40-df6f07cb3011",
   "metadata": {},
   "source": [
    "# Perform a Grid Search with k-Fold Cross Validator to find the best hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fcb895-51af-47ec-af06-889e95777344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up our search grid\n",
    "param_grid = {'max_depth': [2, 3, 4], # default 3\n",
    "              'n_estimators': [80, 100, 120], # default 100\n",
    "              'learning_rate': [0.05, 0.1, 0.2]}  # default 0.1\n",
    "\n",
    "\n",
    "def find_best_hyperparams(X_train, Y_train, param_grid):\n",
    "    #=========================================================================\n",
    "    # XGBoost regression: \n",
    "    # Parameters: \n",
    "    # n_estimators  \"Number of gradient boosted trees. Equivalent to number \n",
    "    #                of boosting rounds.\"\n",
    "    # learning_rate \"Boosting learning rate (also known as “eta”)\"\n",
    "    # max_depth     \"Maximum depth of a tree. Increasing this value will make \n",
    "    #                the model more complex and more likely to overfit.\" \n",
    "    #=========================================================================\n",
    "    regressor=xgb.XGBRegressor(eval_metric='rmse', n_jobs=-1, verbosity=1)\n",
    "\n",
    "    # Create a k-fold split iterator\n",
    "    kf = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "    # try out every combination of the above values\n",
    "    gscv_search = GridSearchCV(regressor,\n",
    "                               param_grid,\n",
    "                               scoring='neg_root_mean_squared_error',\n",
    "                               cv=kf,\n",
    "                               verbose=3,\n",
    "                               n_jobs=1).fit(X_train, Y_train)\n",
    "    \n",
    "    return gscv_search\n",
    "\n",
    "gscv_search = find_best_hyperparams(X_train, Y_train, param_grid)\n",
    "print(\"The best hyperparameters are \", gscv_search.best_params_)\n",
    "\n",
    "# for i, (train_index, test_index) in enumerate(kf.split(X_train)):\n",
    "#     print(f\"Fold {i}:\")\n",
    "#     print(len(train_index))\n",
    "#     print(len(test_index))\n",
    "#     # print(f\"  Train: index={train_index}\")\n",
    "#     # print(f\"  Test:  index={test_index}\")\n",
    "#     X_train_cv, X_test_cv, Y_train_cv, Y_test_cv = X_train.iloc[train_index], X_train.iloc[test_index], Y_train.iloc[train_index], Y_train.iloc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b1b96e-12b7-40c6-92fd-d32514af410f",
   "metadata": {},
   "source": [
    "# Re-train using the best hyper-parameters and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1340755-5506-4093-a708-a789cda3911a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_best_hyperparams(X_train, Y_train, gscv_search):\n",
    "    # fit model to training data using the best parameters of the grid search\n",
    "    model=xgb.XGBRegressor(learning_rate = gscv_search.best_params_['learning_rate'],\n",
    "                           n_estimators  = gscv_search.best_params_['n_estimators'],\n",
    "                           max_depth     = gscv_search.best_params_['max_depth'],\n",
    "                           objective     = 'reg:squarederror',\n",
    "                           eval_metric   = 'rmse',\n",
    "                           n_jobs        = -1,\n",
    "                           verbosity     = 1)\n",
    "\n",
    "    model.fit(X_train, Y_train, verbose=True)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = train_with_best_hyperparams(X_train, Y_train, gscv_search)\n",
    "\n",
    "# Calculate training score\n",
    "train_score = model.score(X_train, Y_train)  \n",
    "print(\"Training score: \", train_score)\n",
    "\n",
    "# Calculate test score\n",
    "test_score = model.score(X_test, Y_test)  \n",
    "print(\"Test score: \", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccddaff5-e090-415b-934c-dee470d35bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "print(Y_test[0])\n",
    "print(Y_pred[0])\n",
    "\n",
    "\n",
    "Y_pred = model.predict(X_train)\n",
    "\n",
    "print(Y_train[0])\n",
    "print(Y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ee0973-c870-45a1-8fa7-74f48a42d21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to JSON\n",
    "model.save_model(fitted_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
