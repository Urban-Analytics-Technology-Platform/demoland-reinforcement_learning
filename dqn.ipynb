{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43c7d051-fa52-45a9-b6f7-18db61c6292b",
   "metadata": {},
   "source": [
    "# Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc70caba-ecc8-4ecd-800c-ea09d372313b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "NVIDIA RTX A1000 Laptop GPU\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import OrderedDict, namedtuple, deque\n",
    "\n",
    "\n",
    "# Get cpu or gpu device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "#Additional info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3a5856-7c5e-40f9-8468-d7d491a95a0f",
   "metadata": {},
   "source": [
    "# DQN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a954d7df-58c2-4324-968e-e02bc991e9af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, hidden_size, action_size, seed):\n",
    "        # weights and bias are initialized from uniform(−sqrt(k),sqrt(k)), where k=1/in_features.\n",
    "        # This is similar, but not same, to Kaiming (He) uniform initialization.\n",
    "        super(DQN, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.model = nn.Sequential(OrderedDict([\n",
    "            ('fc1', nn.Linear(state_size, hidden_size)),   # input  -> hidden\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('fc2', nn.Linear(hidden_size, action_size)),  # hidden -> output\n",
    "        ]))   \n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.model(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc058c4-b3fa-4fb6-9ab4-157813c252bb",
   "metadata": {},
   "source": [
    "# Experience replay buffer\n",
    "\n",
    "Replay buffer and Agent code taken from [here](https://github.com/udacity/deep-reinforcement-learning) (MIT License) and adapted accordingly for our problem (added a valid_actions array since not all actions are valin in every state, added Double DQN functionality, Huber loss and AdamW optimizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66bc3a9f-c0a5-493e-aa3d-b6c3c7ccd4f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \n",
    "    def __init__(self, buffer_size, batch_size, seed):\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d466cfa2-80a6-4043-9803-8b482dbd1f53",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8147acf3-1c88-449b-b978-802fa792a1ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, state_size, hidden_size, action_size, replay_memory_size=1e5, batch_size=64, gamma=0.99,\n",
    "                 learning_rate=1e-3, target_tau=2e-3, update_rate=4, seed=0):\n",
    "        self.state_size = state_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.action_size = action_size\n",
    "        self.buffer_size = int(replay_memory_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.learn_rate = learning_rate\n",
    "        self.tau = target_tau\n",
    "        self.update_rate = update_rate\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "\n",
    "        self.network = DQN(state_size, hidden_size, action_size, seed).to(device)\n",
    "        self.target_network = DQN(state_size, hidden_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.AdamW(self.network.parameters(), lr=self.learn_rate)  # or optim.SGD or optim.Adam\n",
    "        \n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(self.buffer_size, self.batch_size, self.seed)\n",
    "\n",
    "        # Initialize time step (for updating every update_rate steps)\n",
    "        self.t_step = 0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every update_rate time steps.\n",
    "        self.t_step = (self.t_step + 1) % self.update_rate\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, self.gamma)\n",
    "    \n",
    "    def act(self, state, valid_actions, eps=0.0):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.network.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.network(state)\n",
    "        self.network.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            # print(action_values.cpu().data.numpy())\n",
    "            return valid_actions[np.argmax(action_values.cpu().data.numpy()[0][valid_actions])]\n",
    "        else:\n",
    "            return random.choice(valid_actions)\n",
    "    \n",
    "    def learn(self, experiences, gamma):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Get Q values from current observations (s, a) using model nextwork\n",
    "        Qsa = self.network(states).gather(1, actions)\n",
    "        \n",
    "        #Double DQN\n",
    "        Qsa_prime_actions = self.network(next_states).detach().max(1)[1].unsqueeze(1)\n",
    "        Qsa_prime_targets = self.target_network(next_states)[Qsa_prime_actions].unsqueeze(1)\n",
    "        \n",
    "        # Compute Q targets for current states \n",
    "        Qsa_targets = rewards + (gamma * Qsa_prime_targets * (1 - dones))\n",
    "        \n",
    "        # Compute loss (error)\n",
    "        loss = F.huber_loss(Qsa, Qsa_targets)  # or F.mse_loss\n",
    "\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.network, self.target_network, self.tau)\n",
    "    \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"\n",
    "        Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a96e89-25f3-4ddd-9953-4be8a0f523bc",
   "metadata": {},
   "source": [
    "# Environment\n",
    "\n",
    "A very simple environment to test the code while we are waiting for Turing to provide the Black-box.\n",
    "\n",
    "Given an array of variables, we can produce the indicators by feeding them into our own toy black-box. The variables need to be within the range [0, 24] and the black-box just divides each variable by 2 to produce the indicators. We also have a set of actions, where one action is to select a variable and either increase it or decrease it by 1. The goal is, given an array of target indicators and initial variables (from which we can create the initial indicators using the black-box), to perform the required actions to read the target and maximize our reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da7ab87d-6c3d-4519-b16d-a5d2b200d79e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    \n",
    "    def __init__(self, initial_variables, target_indicators):\n",
    "        self.variables = initial_variables\n",
    "        self.target = target_indicators\n",
    "        self.indicators = self.get_indicators()\n",
    "        self.actions = np.arange(initial_variables.size * 2, dtype=np.int32)  # each variable +-\n",
    "        self.valid_actions = self.get_valid_actions()\n",
    "        self.state = self.indicators - self.target\n",
    "        if np.linalg.norm(self.state) < 1e-5:\n",
    "            self.done = 1\n",
    "        else:\n",
    "            self.done = 0\n",
    "        self.episode = {'variables': [self.variables],\n",
    "                        'indicators': [self.indicators],\n",
    "                        'valid_actions': [self.valid_actions],\n",
    "                        'state': [self.state],\n",
    "                        'done': [self.done],\n",
    "                        'action': [],\n",
    "                        'reward': []}\n",
    "    \n",
    "    def get_indicators(self):\n",
    "        indicators = 0.5 * self.variables  # black-box here\n",
    "        return indicators\n",
    "    \n",
    "    def get_valid_actions(self):\n",
    "        \"\"\"\n",
    "        [var1+, var1-, var2+, var2-, ..., varN+, varN-]\n",
    "        \"\"\"\n",
    "        min_variable_val = 0.0\n",
    "        max_variable_val = 24.0\n",
    "        \n",
    "        valid_actions = []\n",
    "        for action in self.actions:\n",
    "            var_id = action // 2\n",
    "            \n",
    "            if action % 2 == 0:\n",
    "                if self.variables[var_id] < max_variable_val:\n",
    "                    valid_actions.append(action)\n",
    "            elif action % 2 == 1:\n",
    "                if self.variables[var_id] > min_variable_val:\n",
    "                    valid_actions.append(action)\n",
    "        \n",
    "        return np.array(valid_actions, dtype=np.int32)\n",
    "    \n",
    "    def get_reward(self):\n",
    "        prev_state_norm = np.linalg.norm(self.episode['state'][-2])\n",
    "        curr_state_norm = np.linalg.norm(self.episode['state'][-1])\n",
    "        reward = prev_state_norm - curr_state_norm\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def step(self, action):\n",
    "        if action not in self.valid_actions:\n",
    "            print(\"Illegal move\")\n",
    "            print(\"Need debugging. Check agent.act() routine\")\n",
    "        \n",
    "        var_id = action // 2\n",
    "        if action % 2 == 0:\n",
    "            self.variables[var_id] += 1  # +1 action of this variable\n",
    "        elif action % 2 == 1:\n",
    "            self.variables[var_id] -= 1  # -1 action of this variable\n",
    "        \n",
    "        self.indicators = self.get_indicators()\n",
    "        self.valid_actions = self.get_valid_actions()\n",
    "        self.state = self.indicators - self.target\n",
    "        if np.linalg.norm(self.state) < 1e-5:\n",
    "            self.done = 1\n",
    "        else:\n",
    "            self.done = 0\n",
    "        self.episode['variables'].append(self.variables)\n",
    "        self.episode['indicators'].append(self.indicators)\n",
    "        self.episode['valid_actions'].append(self.valid_actions)\n",
    "        self.episode['state'].append(self.state)\n",
    "        self.episode['done'].append(self.done)\n",
    "        \n",
    "        self.episode['action'].append(action)\n",
    "        \n",
    "        reward = self.get_reward()\n",
    "        self.episode['reward'].append(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0435ae2-a621-47ae-aeb0-e558d1639d1e",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e73b34db-d3ef-47c6-b10d-ab55e2eca847",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_912/3427499130.py:67: UserWarning: Using a target size (torch.Size([32, 1, 32, 16])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.huber_loss(Qsa, Qsa_targets)  # or F.mse_loss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20\tAverage Score: 2.76\n",
      "Episode 40\tAverage Score: 6.39\n",
      "Episode 60\tAverage Score: 7.72\n",
      "Episode 80\tAverage Score: 7.82\n",
      "Episode 100\tAverage Score: 7.89\n",
      "Episode 120\tAverage Score: 7.89\n",
      "Episode 140\tAverage Score: 7.89\n",
      "Episode 160\tAverage Score: 7.89\n",
      "Episode 180\tAverage Score: 7.89\n",
      "Episode 200\tAverage Score: 7.89\n"
     ]
    }
   ],
   "source": [
    "# random.seed(1)  # doesn't work yet\n",
    "\n",
    "\n",
    "num_episodes = 200\n",
    "max_num_steps_per_episode = 300\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.05\n",
    "epsilon_decay = 0.99\n",
    "scores = []\n",
    "scores_average_window = 20\n",
    "\n",
    "state_size = 8\n",
    "action_size = state_size * 2\n",
    "\n",
    "agent = Agent(state_size=state_size, hidden_size=action_size, action_size=action_size, replay_memory_size=3000, batch_size=32,\n",
    "              gamma=0.99, learning_rate=1e-2, target_tau=4e-2, update_rate=8, seed=0)\n",
    "\n",
    "for i_episode in range(1, num_episodes+1):\n",
    "    # reset the environment\n",
    "    initial_variables = np.array([1.0, 0.0, 24.0, 13.0, 7.0, 23.0, 24.0, 2.0], dtype=np.float32)\n",
    "    target_indicators = np.array([2.5, 0.5, 10.5, 7.0, 3.5, 5.0, 11.0, 4.5], dtype=np.float32)\n",
    "    env = Environment(initial_variables, target_indicators)\n",
    "    \n",
    "    # get initial state of the unity environment \n",
    "    state = env.episode['state'][-1]\n",
    "    \n",
    "    done = env.episode['done'][-1]\n",
    "    if done:\n",
    "        print('Episode completed')\n",
    "        continue\n",
    "    \n",
    "    # set the initial episode score to zero.\n",
    "    score = 0\n",
    "    \n",
    "    for i_step in range(1, max_num_steps_per_episode+1):\n",
    "        valid_actions = env.episode['valid_actions'][-1]\n",
    "        \n",
    "        # determine epsilon-greedy action from current sate\n",
    "        action = agent.act(state, valid_actions, epsilon)\n",
    "        \n",
    "        # send the action to the environment\n",
    "        env.step(action)\n",
    "        \n",
    "        next_state = env.episode['state'][-1]    # get the next state\n",
    "        reward = env.episode['reward'][-1]       # get the reward\n",
    "        done = env.episode['done'][-1]           # see if episode has finished\n",
    "        \n",
    "        #Send (S, A, R, S') info to the DQN agent for a neural network update\n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "        \n",
    "        # set new state to current state for determining next action\n",
    "        state = next_state\n",
    "\n",
    "        # Update episode score\n",
    "        score += reward\n",
    "        \n",
    "        # If this episode is done, \n",
    "        # then exit episode loop, to begin new episode\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        \n",
    "    # Add episode score to Scores and...\n",
    "    # Calculate mean score over last 100 episodes \n",
    "    # Mean score is calculated over current episodes until i_episode > 100\n",
    "    scores.append(score)\n",
    "    average_score = np.mean(scores[i_episode-min(i_episode, scores_average_window):i_episode+1])\n",
    "\n",
    "    # Decrease epsilon for epsilon-greedy policy by decay rate\n",
    "    # Use max method to make sure epsilon doesn't decrease below epsilon_min\n",
    "    epsilon = max(epsilon_min, epsilon_decay*epsilon)\n",
    "\n",
    "    # (Over-) Print current average score\n",
    "    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, average_score), end=\"\")\n",
    "\n",
    "    # Print average score every scores_average_window episodes\n",
    "    if i_episode % scores_average_window == 0:\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, average_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970f2f23-5f01-4741-8579-072ce2ca30a2",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "\n",
    "Using the agent that we trained before should produce a set of actions that reach the target in the best way (in this case this will be with the least number of actions but in general the agent will try to maximize the expected reward).\n",
    "\n",
    "We can also create a new agent, that will be initialized with random weights and biases. Using this for inference should produce random actions and so the agent will stop after the pre defined maximum number of actions per episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d75160d2-db2c-4295-8418-38f7c4d94ac8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target indicators:  [ 2.5  0.5 10.5  7.   3.5  5.  11.   4.5]\n",
      "\n",
      "Current indicators: [ 0.5  0.  12.   6.5  3.5 11.5 12.   1. ]\n",
      "Current indicators: [ 0.5  0.  12.   6.5  3.5 11.  12.   1. ]\n",
      "Current indicators: [ 0.5  0.  12.   6.5  3.5 10.5 12.   1. ]\n",
      "Current indicators: [ 0.5  0.  12.   6.5  3.5 10.  12.   1. ]\n",
      "Current indicators: [ 0.5  0.  12.   6.5  3.5  9.5 12.   1. ]\n",
      "Current indicators: [ 0.5  0.  12.   6.5  3.5  9.5 12.   1.5]\n",
      "Current indicators: [ 0.5  0.  12.   6.5  3.5  9.  12.   1.5]\n",
      "Current indicators: [ 0.5  0.  12.   6.5  3.5  8.5 12.   1.5]\n",
      "Current indicators: [ 0.5  0.  12.   6.5  3.5  8.5 12.   2. ]\n",
      "Current indicators: [ 0.5  0.  12.   6.5  3.5  8.5 12.   2.5]\n",
      "Current indicators: [ 0.5  0.  12.   6.5  3.5  8.  12.   2.5]\n",
      "Current indicators: [ 0.5  0.  12.   6.5  3.5  8.  12.   3. ]\n",
      "Current indicators: [ 0.5  0.  12.   6.5  3.5  7.5 12.   3. ]\n",
      "Current indicators: [ 1.   0.  12.   6.5  3.5  7.5 12.   3. ]\n",
      "Current indicators: [ 1.   0.  12.   6.5  3.5  7.  12.   3. ]\n",
      "Current indicators: [ 1.   0.  12.   6.5  3.5  7.  12.   3.5]\n",
      "Current indicators: [ 1.   0.  12.   6.5  3.5  6.5 12.   3.5]\n",
      "Current indicators: [ 1.   0.  12.   6.5  3.5  6.5 12.   4. ]\n",
      "Current indicators: [ 1.5  0.  12.   6.5  3.5  6.5 12.   4. ]\n",
      "Current indicators: [ 1.5  0.  12.   6.5  3.5  6.  12.   4. ]\n",
      "Current indicators: [ 1.5  0.  11.5  6.5  3.5  6.  12.   4. ]\n",
      "Current indicators: [ 1.5  0.  11.5  6.5  3.5  5.5 12.   4. ]\n",
      "Current indicators: [ 1.5  0.  11.   6.5  3.5  5.5 12.   4. ]\n",
      "Current indicators: [ 1.5  0.  11.   6.5  3.5  5.5 12.   4.5]\n",
      "Current indicators: [ 2.   0.  11.   6.5  3.5  5.5 12.   4.5]\n",
      "Current indicators: [ 2.   0.  11.   6.5  3.5  5.5 11.5  4.5]\n",
      "Current indicators: [ 2.   0.  10.5  6.5  3.5  5.5 11.5  4.5]\n",
      "Current indicators: [ 2.   0.5 10.5  6.5  3.5  5.5 11.5  4.5]\n",
      "Current indicators: [ 2.   0.5 10.5  6.5  3.5  5.5 11.   4.5]\n",
      "Current indicators: [ 2.   0.5 10.5  6.5  3.5  5.  11.   4.5]\n",
      "Current indicators: [ 2.5  0.5 10.5  6.5  3.5  5.  11.   4.5]\n",
      "Current indicators: [ 2.5  0.5 10.5  7.   3.5  5.  11.   4.5]\n"
     ]
    }
   ],
   "source": [
    "state_size = 8\n",
    "action_size = state_size * 2\n",
    "\n",
    "initial_variables = np.array([1.0, 0.0, 24.0, 13.0, 7.0, 23.0, 24.0, 2.0], dtype=np.float32)\n",
    "target_indicators = np.array([2.5, 0.5, 10.5, 7.0, 3.5, 5.0, 11.0, 4.5], dtype=np.float32)\n",
    "env = Environment(initial_variables, target_indicators)\n",
    "\n",
    "print(f\"Target indicators:  {target_indicators}\")\n",
    "print()\n",
    "\n",
    "agent2 = Agent(state_size=state_size, hidden_size=action_size, action_size=action_size)\n",
    "\n",
    "\n",
    "# get initial state of the unity environment \n",
    "state = env.episode['state'][-1]\n",
    "\n",
    "done = env.episode['done'][-1]\n",
    "if done:\n",
    "    print('Episode completed')\n",
    "\n",
    "print(f\"Current indicators: {env.episode['indicators'][-1]}\")\n",
    "    \n",
    "for i_step in range(1, max_num_steps_per_episode+1):\n",
    "    valid_actions = env.episode['valid_actions'][-1]\n",
    "\n",
    "    # determine epsilon-greedy action from current sate\n",
    "    action = agent.act(state, valid_actions)  # trained\n",
    "    # action = agent2.act(state, valid_actions)  # random\n",
    "\n",
    "    # send the action to the environment\n",
    "    env.step(action)\n",
    "    \n",
    "    print(f\"Current indicators: {env.episode['indicators'][-1]}\")\n",
    "\n",
    "    next_state = env.episode['state'][-1]    # get the next state\n",
    "    reward = env.episode['reward'][-1]       # get the reward\n",
    "    done = env.episode['done'][-1]           # see if episode has finished\n",
    "\n",
    "    # set new state to current state for determining next action\n",
    "    state = next_state\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "# print(f\"Target indicators: {target_indicators}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
